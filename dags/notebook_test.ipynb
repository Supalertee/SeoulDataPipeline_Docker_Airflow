{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the PySpark Session ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/supalertee/Projects/venv-main/lib/python3.12/site-packages (from pyarrow) (1.26.4)\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import Column\n",
    "import pyspark.pandas as ps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('practise').getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/supalertee/Projects/python_project/Seoul_analysis_docker/dags'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from pyspark.sql.functions import col, split, trim, substring \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_pollution = spark.read.option('header','true').csv('AirPollutionSeoul/Measurement_summary.csv')\n",
    "df_pyspark_bike = spark.read.option('header','true').csv('SeoulBikeData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_pollution = spark.read.csv('AirPollutionSeoul/Measurement_summary.csv', header= True, inferSchema= True)\n",
    "df_pyspark_bike = spark.read.option('header','true').csv('SeoulBikeData.csv', header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_pollution = df_pyspark_pollution.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 10)).withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2))).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(�C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(�C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>397.458333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-2.454167</td>\n",
       "      <td>45.875000</td>\n",
       "      <td>1.537500</td>\n",
       "      <td>1870.750000</td>\n",
       "      <td>-13.545833</td>\n",
       "      <td>0.248750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <td>355.125000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>61.958333</td>\n",
       "      <td>1.712500</td>\n",
       "      <td>1471.083333</td>\n",
       "      <td>-5.716667</td>\n",
       "      <td>0.263750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>300.916667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>81.541667</td>\n",
       "      <td>1.612500</td>\n",
       "      <td>455.750000</td>\n",
       "      <td>1.883333</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>363.708333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-0.304167</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>1362.833333</td>\n",
       "      <td>-9.925000</td>\n",
       "      <td>0.282917</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <td>346.125000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-4.458333</td>\n",
       "      <td>36.416667</td>\n",
       "      <td>1.108333</td>\n",
       "      <td>1959.458333</td>\n",
       "      <td>-17.425000</td>\n",
       "      <td>0.035833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>715.083333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.320833</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>1.029167</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>678.416667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>405.291667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>688.500000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>5.304167</td>\n",
       "      <td>25.791667</td>\n",
       "      <td>1.695833</td>\n",
       "      <td>1429.083333</td>\n",
       "      <td>-13.350000</td>\n",
       "      <td>0.403333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>684.291667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.304167</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.212500</td>\n",
       "      <td>1596.708333</td>\n",
       "      <td>-9.808333</td>\n",
       "      <td>0.117083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>679.041667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.762500</td>\n",
       "      <td>47.208333</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>1581.916667</td>\n",
       "      <td>-8.370833</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rented Bike Count  Hour  Temperature(�C)  Humidity(%)  \\\n",
       "Date                                                                \n",
       "2017-12-01         397.458333  11.5        -2.454167    45.875000   \n",
       "2017-12-02         355.125000  11.5         1.325000    61.958333   \n",
       "2017-12-03         300.916667  11.5         4.875000    81.541667   \n",
       "2017-12-04         363.708333  11.5        -0.304167    52.500000   \n",
       "2017-12-05         346.125000  11.5        -4.458333    36.416667   \n",
       "...                       ...   ...              ...          ...   \n",
       "2018-11-26         715.083333  11.5         6.320833    70.500000   \n",
       "2018-11-27         678.416667  11.5         7.066667    68.000000   \n",
       "2018-11-28         688.500000  11.5         5.304167    25.791667   \n",
       "2018-11-29         684.291667  11.5         3.304167    38.000000   \n",
       "2018-11-30         679.041667  11.5         2.762500    47.208333   \n",
       "\n",
       "            Wind speed (m/s)  Visibility (10m)  Dew point temperature(�C)  \\\n",
       "Date                                                                        \n",
       "2017-12-01          1.537500       1870.750000                 -13.545833   \n",
       "2017-12-02          1.712500       1471.083333                  -5.716667   \n",
       "2017-12-03          1.612500        455.750000                   1.883333   \n",
       "2017-12-04          3.450000       1362.833333                  -9.925000   \n",
       "2017-12-05          1.108333       1959.458333                 -17.425000   \n",
       "...                      ...               ...                        ...   \n",
       "2018-11-26          1.029167        475.000000                   0.845833   \n",
       "2018-11-27          1.350000        405.291667                   1.000000   \n",
       "2018-11-28          1.695833       1429.083333                 -13.350000   \n",
       "2018-11-29          1.212500       1596.708333                  -9.808333   \n",
       "2018-11-30          1.433333       1581.916667                  -8.370833   \n",
       "\n",
       "            Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
       "Date                                                              \n",
       "2017-12-01                 0.248750      0.000000       0.000000  \n",
       "2017-12-02                 0.263750      0.000000       0.000000  \n",
       "2017-12-03                 0.125417      0.166667       0.000000  \n",
       "2017-12-04                 0.282917      0.004167       0.000000  \n",
       "2017-12-05                 0.035833      0.000000       0.000000  \n",
       "...                             ...           ...            ...  \n",
       "2018-11-26                 0.418750      0.000000       0.120833  \n",
       "2018-11-27                 0.192500      0.000000       0.000000  \n",
       "2018-11-28                 0.403333      0.000000       0.000000  \n",
       "2018-11-29                 0.117083      0.000000       0.000000  \n",
       "2018-11-30                 0.425833      0.000000       0.000000  \n",
       "\n",
       "[365 rows x 10 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark_bike.drop([\"Seasons\",\"Holiday\",\"Functioning Day\"],axis = 1).groupby(\"Date\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3068214f0>]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = df_pyspark_pollution.drop([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"],axis = 1).groupby([\"Measurement date\"]).mean().reset_index()\n",
    "plt.plot(test[\"Measurement date\"],test[\"PM2.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark_pollution = spark.read.csv('AirPollutionSeoul/Measurement_summary.csv', header= True, inferSchema= True)\n",
    "df_pyspark_bike = spark.read.option('header','true').csv('SeoulBikeData.csv', header= True, inferSchema= True)\n",
    "df_pyspark_bike = df_pyspark_bike.toPandas()\n",
    "df_pyspark_pollution = df_pyspark_pollution.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 10)).withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n",
    "df_pyspark_pollution = df_pyspark_pollution.toPandas()\n",
    "i = 0\n",
    "while i < len(df_pyspark_bike):\n",
    "    decompose_temp = df_pyspark_bike[\"Date\"][i].split(\"/\")\n",
    "    df_pyspark_bike.loc[i,\"Date\"] = decompose_temp[2]+ '-' + decompose_temp[1] + '-' + decompose_temp[0]\n",
    "    i += 1\n",
    "# visualization \n",
    "polution_time_series = df_pyspark_pollution.drop([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"],axis = 1).groupby([\"Measurement date\"]).mean().reset_index()\n",
    "bike_time_series = df_pyspark_bike.drop([\"Seasons\",\"Holiday\",\"Functioning Day\"],axis = 1).groupby(\"Date\").mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(�C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(�C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>397.458333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-2.454167</td>\n",
       "      <td>45.875000</td>\n",
       "      <td>1.537500</td>\n",
       "      <td>1870.750000</td>\n",
       "      <td>-13.545833</td>\n",
       "      <td>0.248750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>355.125000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>61.958333</td>\n",
       "      <td>1.712500</td>\n",
       "      <td>1471.083333</td>\n",
       "      <td>-5.716667</td>\n",
       "      <td>0.263750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>300.916667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>81.541667</td>\n",
       "      <td>1.612500</td>\n",
       "      <td>455.750000</td>\n",
       "      <td>1.883333</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>363.708333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-0.304167</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>1362.833333</td>\n",
       "      <td>-9.925000</td>\n",
       "      <td>0.282917</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>346.125000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>-4.458333</td>\n",
       "      <td>36.416667</td>\n",
       "      <td>1.108333</td>\n",
       "      <td>1959.458333</td>\n",
       "      <td>-17.425000</td>\n",
       "      <td>0.035833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>715.083333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.320833</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>1.029167</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>678.416667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>405.291667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>688.500000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>5.304167</td>\n",
       "      <td>25.791667</td>\n",
       "      <td>1.695833</td>\n",
       "      <td>1429.083333</td>\n",
       "      <td>-13.350000</td>\n",
       "      <td>0.403333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>684.291667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.304167</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.212500</td>\n",
       "      <td>1596.708333</td>\n",
       "      <td>-9.808333</td>\n",
       "      <td>0.117083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>679.041667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.762500</td>\n",
       "      <td>47.208333</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>1581.916667</td>\n",
       "      <td>-8.370833</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Rented Bike Count  Hour  Temperature(�C)  Humidity(%)  \\\n",
       "0    2017-12-01         397.458333  11.5        -2.454167    45.875000   \n",
       "1    2017-12-02         355.125000  11.5         1.325000    61.958333   \n",
       "2    2017-12-03         300.916667  11.5         4.875000    81.541667   \n",
       "3    2017-12-04         363.708333  11.5        -0.304167    52.500000   \n",
       "4    2017-12-05         346.125000  11.5        -4.458333    36.416667   \n",
       "..          ...                ...   ...              ...          ...   \n",
       "360  2018-11-26         715.083333  11.5         6.320833    70.500000   \n",
       "361  2018-11-27         678.416667  11.5         7.066667    68.000000   \n",
       "362  2018-11-28         688.500000  11.5         5.304167    25.791667   \n",
       "363  2018-11-29         684.291667  11.5         3.304167    38.000000   \n",
       "364  2018-11-30         679.041667  11.5         2.762500    47.208333   \n",
       "\n",
       "     Wind speed (m/s)  Visibility (10m)  Dew point temperature(�C)  \\\n",
       "0            1.537500       1870.750000                 -13.545833   \n",
       "1            1.712500       1471.083333                  -5.716667   \n",
       "2            1.612500        455.750000                   1.883333   \n",
       "3            3.450000       1362.833333                  -9.925000   \n",
       "4            1.108333       1959.458333                 -17.425000   \n",
       "..                ...               ...                        ...   \n",
       "360          1.029167        475.000000                   0.845833   \n",
       "361          1.350000        405.291667                   1.000000   \n",
       "362          1.695833       1429.083333                 -13.350000   \n",
       "363          1.212500       1596.708333                  -9.808333   \n",
       "364          1.433333       1581.916667                  -8.370833   \n",
       "\n",
       "     Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
       "0                   0.248750      0.000000       0.000000  \n",
       "1                   0.263750      0.000000       0.000000  \n",
       "2                   0.125417      0.166667       0.000000  \n",
       "3                   0.282917      0.004167       0.000000  \n",
       "4                   0.035833      0.000000       0.000000  \n",
       "..                       ...           ...            ...  \n",
       "360                 0.418750      0.000000       0.120833  \n",
       "361                 0.192500      0.000000       0.000000  \n",
       "362                 0.403333      0.000000       0.000000  \n",
       "363                 0.117083      0.000000       0.000000  \n",
       "364                 0.425833      0.000000       0.000000  \n",
       "\n",
       "[365 rows x 11 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type 'list; only ps.Series and ps.DataFrame are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpolution_time_series\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbike_time_series\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/venv-main/lib/python3.12/site-packages/pyspark/pandas/namespace.py:2496\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, sort)\u001b[0m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs:\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (Series, DataFrame)):\n\u001b[0;32m-> 2496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2499\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; only ps.Series \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand ps.DataFrame are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(objs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2501\u001b[0m         )\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly can inner (intersect) or outer (union) join the other axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type 'list; only ps.Series and ps.DataFrame are valid"
     ]
    }
   ],
   "source": [
    "result = ps.concat([polution_time_series, bike_time_series], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_psdf = join_psdf = ps.merge(bike_time_series, polution_time_series, \n",
    "                     left_on='Date', right_on='Measurement date', \n",
    "                     how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/supalertee/Projects/venv-main/lib/python3.12/site-packages/pyspark/pandas/namespace.py:1744: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "join_psdf['Date'] = ps.to_datetime(join_psdf['Date'])\n",
    "december_2018_data = join_psdf[join_psdf['Date'].dt.year == 2018]\n",
    "december_2018_data = december_2018_data[december_2018_data['Date'].dt.month == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(�C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(�C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Measurement date</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>724.500000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.183333</td>\n",
       "      <td>68.916667</td>\n",
       "      <td>1.570833</td>\n",
       "      <td>831.833333</td>\n",
       "      <td>9.370833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>-0.034813</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.492167</td>\n",
       "      <td>72.968333</td>\n",
       "      <td>35.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>899.375000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>18.216667</td>\n",
       "      <td>65.541667</td>\n",
       "      <td>2.004167</td>\n",
       "      <td>1099.458333</td>\n",
       "      <td>11.212500</td>\n",
       "      <td>0.589167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>-0.026598</td>\n",
       "      <td>0.026548</td>\n",
       "      <td>0.028157</td>\n",
       "      <td>0.423833</td>\n",
       "      <td>71.190000</td>\n",
       "      <td>38.576667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>875.625000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>17.958333</td>\n",
       "      <td>70.291667</td>\n",
       "      <td>2.112500</td>\n",
       "      <td>1157.625000</td>\n",
       "      <td>12.279167</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.025173</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>68.715000</td>\n",
       "      <td>20.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>833.125000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>13.737500</td>\n",
       "      <td>70.083333</td>\n",
       "      <td>1.825000</td>\n",
       "      <td>1555.750000</td>\n",
       "      <td>7.645833</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>-0.016262</td>\n",
       "      <td>-0.001137</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.358833</td>\n",
       "      <td>12.248333</td>\n",
       "      <td>8.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>108.166667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.795833</td>\n",
       "      <td>87.333333</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>1145.958333</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>0.089583</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>-0.016313</td>\n",
       "      <td>-0.002302</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>6.595000</td>\n",
       "      <td>4.043333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.304167</td>\n",
       "      <td>74.041667</td>\n",
       "      <td>3.037500</td>\n",
       "      <td>848.458333</td>\n",
       "      <td>1.579167</td>\n",
       "      <td>0.466250</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.027327</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>118.576667</td>\n",
       "      <td>28.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>464.958333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1597.958333</td>\n",
       "      <td>-7.237500</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.012657</td>\n",
       "      <td>0.035253</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>49.643333</td>\n",
       "      <td>23.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>277.750000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>56.958333</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>1578.916667</td>\n",
       "      <td>-4.862500</td>\n",
       "      <td>0.330417</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.020183</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.401667</td>\n",
       "      <td>28.478333</td>\n",
       "      <td>14.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>795.083333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.479167</td>\n",
       "      <td>60.291667</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>926.000000</td>\n",
       "      <td>-1.720833</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.040198</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>40.793333</td>\n",
       "      <td>25.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>695.666667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.654167</td>\n",
       "      <td>46.958333</td>\n",
       "      <td>3.433333</td>\n",
       "      <td>1625.416667</td>\n",
       "      <td>2.845833</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>0.027242</td>\n",
       "      <td>0.446333</td>\n",
       "      <td>45.361667</td>\n",
       "      <td>19.396667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.866667</td>\n",
       "      <td>53.375000</td>\n",
       "      <td>3.008333</td>\n",
       "      <td>1331.541667</td>\n",
       "      <td>2.120833</td>\n",
       "      <td>1.035833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.021752</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>62.958333</td>\n",
       "      <td>24.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>947.041667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>13.029167</td>\n",
       "      <td>32.416667</td>\n",
       "      <td>2.575000</td>\n",
       "      <td>1885.750000</td>\n",
       "      <td>-4.641667</td>\n",
       "      <td>1.023333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.028692</td>\n",
       "      <td>0.026863</td>\n",
       "      <td>0.405333</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>26.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2018-04-13</td>\n",
       "      <td>968.333333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.291667</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1997.833333</td>\n",
       "      <td>-7.516667</td>\n",
       "      <td>0.528333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-13</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.024618</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>36.171667</td>\n",
       "      <td>14.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>290.833333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.587500</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>1.829167</td>\n",
       "      <td>1388.791667</td>\n",
       "      <td>4.466667</td>\n",
       "      <td>0.145417</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.031950</td>\n",
       "      <td>0.359333</td>\n",
       "      <td>21.021667</td>\n",
       "      <td>12.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>425.291667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.412500</td>\n",
       "      <td>67.708333</td>\n",
       "      <td>2.337500</td>\n",
       "      <td>1301.125000</td>\n",
       "      <td>3.420833</td>\n",
       "      <td>0.720417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.030470</td>\n",
       "      <td>0.426833</td>\n",
       "      <td>73.856667</td>\n",
       "      <td>21.346667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>966.583333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.429167</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1836.666667</td>\n",
       "      <td>-1.533333</td>\n",
       "      <td>1.032917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.028660</td>\n",
       "      <td>0.026707</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>37.925000</td>\n",
       "      <td>22.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>926.416667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.808333</td>\n",
       "      <td>36.916667</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1622.041667</td>\n",
       "      <td>-2.133333</td>\n",
       "      <td>1.001250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.039682</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>60.121667</td>\n",
       "      <td>29.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>902.916667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.379167</td>\n",
       "      <td>35.416667</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>1497.791667</td>\n",
       "      <td>-3.416667</td>\n",
       "      <td>1.045417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>69.336667</td>\n",
       "      <td>34.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>738.750000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>13.787500</td>\n",
       "      <td>37.541667</td>\n",
       "      <td>1.925000</td>\n",
       "      <td>1163.291667</td>\n",
       "      <td>-1.425000</td>\n",
       "      <td>0.931667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.045260</td>\n",
       "      <td>0.036282</td>\n",
       "      <td>0.552167</td>\n",
       "      <td>79.113333</td>\n",
       "      <td>43.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>941.375000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>45.791667</td>\n",
       "      <td>1.320833</td>\n",
       "      <td>789.708333</td>\n",
       "      <td>5.395833</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.059388</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>0.730833</td>\n",
       "      <td>103.998333</td>\n",
       "      <td>70.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2018-04-21</td>\n",
       "      <td>1016.833333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>18.662500</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>1.829167</td>\n",
       "      <td>1292.708333</td>\n",
       "      <td>-0.129167</td>\n",
       "      <td>1.089167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-21</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.050853</td>\n",
       "      <td>0.026857</td>\n",
       "      <td>0.622667</td>\n",
       "      <td>71.281667</td>\n",
       "      <td>41.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>285.500000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.033333</td>\n",
       "      <td>52.541667</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>1578.916667</td>\n",
       "      <td>5.458333</td>\n",
       "      <td>0.212083</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>0.431333</td>\n",
       "      <td>37.660000</td>\n",
       "      <td>21.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>40.708333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.425000</td>\n",
       "      <td>93.791667</td>\n",
       "      <td>2.208333</td>\n",
       "      <td>721.041667</td>\n",
       "      <td>8.416667</td>\n",
       "      <td>0.107917</td>\n",
       "      <td>2.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.039822</td>\n",
       "      <td>0.343667</td>\n",
       "      <td>10.156667</td>\n",
       "      <td>7.068333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>839.333333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.195833</td>\n",
       "      <td>65.208333</td>\n",
       "      <td>1.412500</td>\n",
       "      <td>1938.458333</td>\n",
       "      <td>5.237500</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>0.002767</td>\n",
       "      <td>0.018143</td>\n",
       "      <td>0.034387</td>\n",
       "      <td>0.325833</td>\n",
       "      <td>8.763333</td>\n",
       "      <td>4.938333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>1095.208333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.245833</td>\n",
       "      <td>38.791667</td>\n",
       "      <td>1.470833</td>\n",
       "      <td>1880.333333</td>\n",
       "      <td>-0.808333</td>\n",
       "      <td>1.040417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.488333</td>\n",
       "      <td>32.726667</td>\n",
       "      <td>23.201667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>1069.583333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.087500</td>\n",
       "      <td>50.083333</td>\n",
       "      <td>1.870833</td>\n",
       "      <td>1355.666667</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.911250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.028923</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>41.320000</td>\n",
       "      <td>31.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>1077.541667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.575000</td>\n",
       "      <td>51.708333</td>\n",
       "      <td>1.704167</td>\n",
       "      <td>985.750000</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.028793</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>59.208333</td>\n",
       "      <td>40.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2018-04-28</td>\n",
       "      <td>1039.166667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.508333</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>2.025000</td>\n",
       "      <td>1153.208333</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-28</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.021997</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>59.368333</td>\n",
       "      <td>38.148333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2018-04-29</td>\n",
       "      <td>1056.208333</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.870833</td>\n",
       "      <td>43.125000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1252.708333</td>\n",
       "      <td>2.441667</td>\n",
       "      <td>1.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-29</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.552333</td>\n",
       "      <td>54.443333</td>\n",
       "      <td>35.463333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>1060.916667</td>\n",
       "      <td>11.5</td>\n",
       "      <td>18.833333</td>\n",
       "      <td>60.375000</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>495.333333</td>\n",
       "      <td>10.691667</td>\n",
       "      <td>0.864167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>0.042458</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>78.655000</td>\n",
       "      <td>56.215000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Rented Bike Count  Hour  Temperature(�C)  Humidity(%)  \\\n",
       "121 2018-04-01         724.500000  11.5        15.183333    68.916667   \n",
       "122 2018-04-02         899.375000  11.5        18.216667    65.541667   \n",
       "123 2018-04-03         875.625000  11.5        17.958333    70.291667   \n",
       "124 2018-04-04         833.125000  11.5        13.737500    70.083333   \n",
       "125 2018-04-05         108.166667  11.5         7.795833    87.333333   \n",
       "126 2018-04-06         480.000000  11.5         6.304167    74.041667   \n",
       "127 2018-04-07         464.958333  11.5         4.208333    46.000000   \n",
       "128 2018-04-08         277.750000  11.5         3.812500    56.958333   \n",
       "129 2018-04-09         795.083333  11.5         7.479167    60.291667   \n",
       "130 2018-04-10         695.666667  11.5        14.654167    46.958333   \n",
       "131 2018-04-11           0.000000  11.5        12.866667    53.375000   \n",
       "132 2018-04-12         947.041667  11.5        13.029167    32.416667   \n",
       "133 2018-04-13         968.333333  11.5        14.291667    22.250000   \n",
       "134 2018-04-14         290.833333  11.5         9.587500    75.166667   \n",
       "135 2018-04-15         425.291667  11.5         9.412500    67.708333   \n",
       "136 2018-04-16         966.583333  11.5        11.429167    43.875000   \n",
       "137 2018-04-17         926.416667  11.5        12.808333    36.916667   \n",
       "138 2018-04-18         902.916667  11.5        12.379167    35.416667   \n",
       "139 2018-04-19         738.750000  11.5        13.787500    37.541667   \n",
       "140 2018-04-20         941.375000  11.5        18.062500    45.791667   \n",
       "141 2018-04-21        1016.833333  11.5        18.662500    35.333333   \n",
       "142 2018-04-22         285.500000  11.5        16.033333    52.541667   \n",
       "143 2018-04-23          40.708333  11.5         9.425000    93.791667   \n",
       "144 2018-04-24         839.333333  11.5        12.195833    65.208333   \n",
       "145 2018-04-25        1095.208333  11.5        14.245833    38.791667   \n",
       "146 2018-04-26        1069.583333  11.5        15.087500    50.083333   \n",
       "147 2018-04-27        1077.541667  11.5        15.575000    51.708333   \n",
       "148 2018-04-28        1039.166667  11.5        15.508333    50.500000   \n",
       "149 2018-04-29        1056.208333  11.5        15.870833    43.125000   \n",
       "150 2018-04-30        1060.916667  11.5        18.833333    60.375000   \n",
       "\n",
       "     Wind speed (m/s)  Visibility (10m)  Dew point temperature(�C)  \\\n",
       "121          1.570833        831.833333                   9.370833   \n",
       "122          2.004167       1099.458333                  11.212500   \n",
       "123          2.112500       1157.625000                  12.279167   \n",
       "124          1.825000       1555.750000                   7.645833   \n",
       "125          1.987500       1145.958333                   5.575000   \n",
       "126          3.037500        848.458333                   1.579167   \n",
       "127          4.000000       1597.958333                  -7.237500   \n",
       "128          1.733333       1578.916667                  -4.862500   \n",
       "129          1.050000        926.000000                  -1.720833   \n",
       "130          3.433333       1625.416667                   2.845833   \n",
       "131          3.008333       1331.541667                   2.120833   \n",
       "132          2.575000       1885.750000                  -4.641667   \n",
       "133          1.400000       1997.833333                  -7.516667   \n",
       "134          1.829167       1388.791667                   4.466667   \n",
       "135          2.337500       1301.125000                   3.420833   \n",
       "136          1.600000       1836.666667                  -1.533333   \n",
       "137          1.812500       1622.041667                  -2.133333   \n",
       "138          1.941667       1497.791667                  -3.416667   \n",
       "139          1.925000       1163.291667                  -1.425000   \n",
       "140          1.320833        789.708333                   5.395833   \n",
       "141          1.829167       1292.708333                  -0.129167   \n",
       "142          1.066667       1578.916667                   5.458333   \n",
       "143          2.208333        721.041667                   8.416667   \n",
       "144          1.412500       1938.458333                   5.237500   \n",
       "145          1.470833       1880.333333                  -0.808333   \n",
       "146          1.870833       1355.666667                   3.875000   \n",
       "147          1.704167        985.750000                   5.133333   \n",
       "148          2.025000       1153.208333                   3.875000   \n",
       "149          1.900000       1252.708333                   2.441667   \n",
       "150          1.587500        495.333333                  10.691667   \n",
       "\n",
       "     Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm) Measurement date  \\\n",
       "121                 0.302083      0.000000            0.0       2018-04-01   \n",
       "122                 0.589167      0.000000            0.0       2018-04-02   \n",
       "123                 0.286667      0.000000            0.0       2018-04-03   \n",
       "124                 0.815000      0.775000            0.0       2018-04-04   \n",
       "125                 0.089583      0.416667            0.0       2018-04-05   \n",
       "126                 0.466250      0.291667            0.0       2018-04-06   \n",
       "127                 0.760000      0.008333            0.0       2018-04-07   \n",
       "128                 0.330417      0.125000            0.0       2018-04-08   \n",
       "129                 0.896667      0.000000            0.0       2018-04-09   \n",
       "130                 0.730000      0.208333            0.0       2018-04-10   \n",
       "131                 1.035833      0.000000            0.0       2018-04-11   \n",
       "132                 1.023333      0.000000            0.0       2018-04-12   \n",
       "133                 0.528333      0.000000            0.0       2018-04-13   \n",
       "134                 0.145417      0.375000            0.0       2018-04-14   \n",
       "135                 0.720417      0.000000            0.0       2018-04-15   \n",
       "136                 1.032917      0.000000            0.0       2018-04-16   \n",
       "137                 1.001250      0.000000            0.0       2018-04-17   \n",
       "138                 1.045417      0.000000            0.0       2018-04-18   \n",
       "139                 0.931667      0.000000            0.0       2018-04-19   \n",
       "140                 0.945000      0.000000            0.0       2018-04-20   \n",
       "141                 1.089167      0.000000            0.0       2018-04-21   \n",
       "142                 0.212083      0.562500            0.0       2018-04-22   \n",
       "143                 0.107917      2.541667            0.0       2018-04-23   \n",
       "144                 0.422500      0.125000            0.0       2018-04-24   \n",
       "145                 1.040417      0.000000            0.0       2018-04-25   \n",
       "146                 0.911250      0.000000            0.0       2018-04-26   \n",
       "147                 0.954167      0.000000            0.0       2018-04-27   \n",
       "148                 1.085000      0.000000            0.0       2018-04-28   \n",
       "149                 1.020833      0.000000            0.0       2018-04-29   \n",
       "150                 0.864167      0.000000            0.0       2018-04-30   \n",
       "\n",
       "          SO2       NO2        O3        CO        PM10      PM2.5  \n",
       "121 -0.034813  0.027888  0.028333  0.492167   72.968333  35.460000  \n",
       "122 -0.026598  0.026548  0.028157  0.423833   71.190000  38.576667  \n",
       "123  0.005143  0.025173  0.020373  0.413333   68.715000  20.883333  \n",
       "124 -0.016262 -0.001137  0.007723  0.358833   12.248333   8.365000  \n",
       "125 -0.016313 -0.002302  0.001002  0.450500    6.595000   4.043333  \n",
       "126  0.003755  0.017685  0.027327  0.510000  118.576667  28.115000  \n",
       "127  0.003255  0.012657  0.035253  0.348000   49.643333  23.840000  \n",
       "128  0.003525  0.020183  0.029392  0.401667   28.478333  14.893333  \n",
       "129  0.003515  0.040198  0.013695  0.550500   40.793333  25.493333  \n",
       "130  0.003795  0.028818  0.027242  0.446333   45.361667  19.396667  \n",
       "131  0.002620  0.021752  0.030262  0.396667   62.958333  24.200000  \n",
       "132  0.004530  0.028692  0.026863  0.405333   45.560000  26.405000  \n",
       "133  0.003723  0.032313  0.024618  0.425833   36.171667  14.713333  \n",
       "134  0.003083  0.017825  0.031950  0.359333   21.021667  12.030000  \n",
       "135  0.003395  0.016763  0.030470  0.426833   73.856667  21.346667  \n",
       "136  0.003538  0.028660  0.026707  0.467000   37.925000  22.570000  \n",
       "137  0.004263  0.039682  0.026715  0.524500   60.121667  29.883333  \n",
       "138  0.004512  0.041952  0.031438  0.498500   69.336667  34.965000  \n",
       "139  0.006310  0.045260  0.036282  0.552167   79.113333  43.525000  \n",
       "140  0.006983  0.059388  0.036390  0.730833  103.998333  70.026667  \n",
       "141  0.005568  0.050853  0.026857  0.622667   71.281667  41.270000  \n",
       "142  0.003753  0.028158  0.026775  0.431333   37.660000  21.183333  \n",
       "143  0.002710  0.015600  0.039822  0.343667   10.156667   7.068333  \n",
       "144  0.002767  0.018143  0.034387  0.325833    8.763333   4.938333  \n",
       "145  0.003325  0.035448  0.024265  0.488333   32.726667  23.201667  \n",
       "146  0.002622  0.034678  0.028923  0.528000   41.320000  31.138333  \n",
       "147  0.005800  0.038728  0.028793  0.518000   59.208333  40.991667  \n",
       "148  0.005153  0.036000  0.021997  0.522000   59.368333  38.148333  \n",
       "149  0.005443  0.031000  0.036558  0.552333   54.443333  35.463333  \n",
       "150  0.005758  0.032055  0.042458  0.641000   78.655000  56.215000  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "december_2018_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3089c27b0>]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 18:35:23 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1075421 ms exceeds timeout 120000 ms\n",
      "24/09/07 18:35:23 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "24/09/07 18:35:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 18:35:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 19:06:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 19:06:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 19:41:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 19:41:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:07:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:07:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:07:11 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:07:11 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:39:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:39:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 20:57:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 20:57:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 21:06:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:06:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:06:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:06:31 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:06:41 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:06:41 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:07:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:07:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:24:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:24:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:24:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 21:24:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/07 21:40:33 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 21:40:33 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 22:08:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 22:08:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 22:41:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 22:41:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:09:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:09:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:25:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:25:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:59:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/07 23:59:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 00:10:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 00:10:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 00:26:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 00:26:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 00:59:42 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 00:59:42 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 01:11:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 01:11:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 01:43:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 01:43:51 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:16:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:16:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:37:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:37:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:54:52 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 02:54:52 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 03:28:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 03:28:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 04:00:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 04:00:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 04:35:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 04:35:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 05:06:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 05:06:31 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 05:19:33 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 05:19:33 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 05:52:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 05:52:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 06:01:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 06:01:03 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 06:18:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 06:18:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 06:51:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 06:51:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 07:24:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 07:24:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 07:55:51 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 07:55:51 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 08:02:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 08:02:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 08:39:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 08:39:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 09:03:03 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 09:03:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 09:51:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 09:51:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 09:53:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 09:53:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 10:24:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 10:24:03 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 10:58:13 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 10:58:13 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 11:05:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 11:05:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 11:35:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 11:35:39 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 11:51:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 11:51:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 12:22:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 12:22:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 13:00:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 13:00:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 13:07:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 13:07:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 13:32:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 13:32:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 14:06:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 14:06:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 14:32:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 14:32:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 14:37:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 14:37:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 14:52:33 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 14:52:33 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 15:13:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 15:13:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 15:13:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 15:13:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 15:13:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 15:13:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 17 more\n",
      "24/09/08 15:25:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 15:25:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.29.74.241:62919\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/09/08 15:25:34 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "# fig,axes = plt.subplots(figsize=(6, 6), ncols=2, nrows=1,layout=\"compressed\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(december_2018_data[\"Date\"],december_2018_data[\"CO\"])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(december_2018_data[\"Date\"],december_2018_data[\"Rented Bike Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/12/2017'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"Date\"][8759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(test):\n",
    "    decompose_temp = test[\"Date\"][i].split(\"/\")\n",
    "    test.loc[i,\"Date\"] = decompose_temp[2]+ '-' + decompose_temp[1] + '-' + decompose_temp[0]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(�C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(�C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Functioning Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>-6</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>-6</td>\n",
       "      <td>36</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1003</td>\n",
       "      <td>19</td>\n",
       "      <td>4.2</td>\n",
       "      <td>34</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>764</td>\n",
       "      <td>20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>37</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>694</td>\n",
       "      <td>21</td>\n",
       "      <td>2.6</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1968</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>712</td>\n",
       "      <td>22</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1859</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>584</td>\n",
       "      <td>23</td>\n",
       "      <td>1.9</td>\n",
       "      <td>43</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1909</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Rented Bike Count Hour Temperature(�C) Humidity(%)  \\\n",
       "0     2017-12-01               254    0            -5.2          37   \n",
       "1     2017-12-01               204    1            -5.5          38   \n",
       "2     2017-12-01               173    2              -6          39   \n",
       "3     2017-12-01               107    3            -6.2          40   \n",
       "4     2017-12-01                78    4              -6          36   \n",
       "...          ...               ...  ...             ...         ...   \n",
       "8755  2018-11-30              1003   19             4.2          34   \n",
       "8756  2018-11-30               764   20             3.4          37   \n",
       "8757  2018-11-30               694   21             2.6          39   \n",
       "8758  2018-11-30               712   22             2.1          41   \n",
       "8759  2018-11-30               584   23             1.9          43   \n",
       "\n",
       "     Wind speed (m/s) Visibility (10m) Dew point temperature(�C)  \\\n",
       "0                 2.2             2000                     -17.6   \n",
       "1                 0.8             2000                     -17.6   \n",
       "2                   1             2000                     -17.7   \n",
       "3                 0.9             2000                     -17.6   \n",
       "4                 2.3             2000                     -18.6   \n",
       "...               ...              ...                       ...   \n",
       "8755              2.6             1894                     -10.3   \n",
       "8756              2.3             2000                      -9.9   \n",
       "8757              0.3             1968                      -9.9   \n",
       "8758                1             1859                      -9.8   \n",
       "8759              1.3             1909                      -9.3   \n",
       "\n",
       "     Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons     Holiday  \\\n",
       "0                          0            0             0  Winter  No Holiday   \n",
       "1                          0            0             0  Winter  No Holiday   \n",
       "2                          0            0             0  Winter  No Holiday   \n",
       "3                          0            0             0  Winter  No Holiday   \n",
       "4                          0            0             0  Winter  No Holiday   \n",
       "...                      ...          ...           ...     ...         ...   \n",
       "8755                       0            0             0  Autumn  No Holiday   \n",
       "8756                       0            0             0  Autumn  No Holiday   \n",
       "8757                       0            0             0  Autumn  No Holiday   \n",
       "8758                       0            0             0  Autumn  No Holiday   \n",
       "8759                       0            0             0  Autumn  No Holiday   \n",
       "\n",
       "     Functioning Day  \n",
       "0                Yes  \n",
       "1                Yes  \n",
       "2                Yes  \n",
       "3                Yes  \n",
       "4                Yes  \n",
       "...              ...  \n",
       "8755             Yes  \n",
       "8756             Yes  \n",
       "8757             Yes  \n",
       "8758             Yes  \n",
       "8759             Yes  \n",
       "\n",
       "[8760 rows x 14 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seoul_YMD =  df_pyspark_pollution.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 10)).withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Measurement date|\n",
      "+----------------+\n",
      "|2017-01-01 00:00|\n",
      "|2017-01-01 01:00|\n",
      "|2017-01-01 02:00|\n",
      "|2017-01-01 03:00|\n",
      "|2017-01-01 04:00|\n",
      "|2017-01-01 05:00|\n",
      "|2017-01-01 06:00|\n",
      "|2017-01-01 07:00|\n",
      "|2017-01-01 08:00|\n",
      "|2017-01-01 09:00|\n",
      "|2017-01-01 10:00|\n",
      "|2017-01-01 11:00|\n",
      "|2017-01-01 12:00|\n",
      "|2017-01-01 13:00|\n",
      "|2017-01-01 14:00|\n",
      "|2017-01-01 15:00|\n",
      "|2017-01-01 16:00|\n",
      "|2017-01-01 17:00|\n",
      "|2017-01-01 18:00|\n",
      "|2017-01-01 19:00|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-12-01']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decompose = test[\"Date\"][1].split(\"/\")\n",
    "test_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decompose[0], test_decompose[2] = test_decompose[2] ,test_decompose[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-12-01'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decompose[0]+ '-' + test_decompose[1] + '-' + test_decompose[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Date\"][1] = test_decompose[0]+ '-' + test_decompose[1] + '-' + test_decompose[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(�C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(�C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Functioning Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>-6</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>-6</td>\n",
       "      <td>36</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>30/11/2018</td>\n",
       "      <td>1003</td>\n",
       "      <td>19</td>\n",
       "      <td>4.2</td>\n",
       "      <td>34</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>30/11/2018</td>\n",
       "      <td>764</td>\n",
       "      <td>20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>37</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>30/11/2018</td>\n",
       "      <td>694</td>\n",
       "      <td>21</td>\n",
       "      <td>2.6</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1968</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>30/11/2018</td>\n",
       "      <td>712</td>\n",
       "      <td>22</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1859</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>30/11/2018</td>\n",
       "      <td>584</td>\n",
       "      <td>23</td>\n",
       "      <td>1.9</td>\n",
       "      <td>43</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1909</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Rented Bike Count Hour Temperature(�C) Humidity(%)  \\\n",
       "0     01/12/2017               254    0            -5.2          37   \n",
       "1     2017-12-01               204    1            -5.5          38   \n",
       "2     01/12/2017               173    2              -6          39   \n",
       "3     01/12/2017               107    3            -6.2          40   \n",
       "4     01/12/2017                78    4              -6          36   \n",
       "...          ...               ...  ...             ...         ...   \n",
       "8755  30/11/2018              1003   19             4.2          34   \n",
       "8756  30/11/2018               764   20             3.4          37   \n",
       "8757  30/11/2018               694   21             2.6          39   \n",
       "8758  30/11/2018               712   22             2.1          41   \n",
       "8759  30/11/2018               584   23             1.9          43   \n",
       "\n",
       "     Wind speed (m/s) Visibility (10m) Dew point temperature(�C)  \\\n",
       "0                 2.2             2000                     -17.6   \n",
       "1                 0.8             2000                     -17.6   \n",
       "2                   1             2000                     -17.7   \n",
       "3                 0.9             2000                     -17.6   \n",
       "4                 2.3             2000                     -18.6   \n",
       "...               ...              ...                       ...   \n",
       "8755              2.6             1894                     -10.3   \n",
       "8756              2.3             2000                      -9.9   \n",
       "8757              0.3             1968                      -9.9   \n",
       "8758                1             1859                      -9.8   \n",
       "8759              1.3             1909                      -9.3   \n",
       "\n",
       "     Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons     Holiday  \\\n",
       "0                          0            0             0  Winter  No Holiday   \n",
       "1                          0            0             0  Winter  No Holiday   \n",
       "2                          0            0             0  Winter  No Holiday   \n",
       "3                          0            0             0  Winter  No Holiday   \n",
       "4                          0            0             0  Winter  No Holiday   \n",
       "...                      ...          ...           ...     ...         ...   \n",
       "8755                       0            0             0  Autumn  No Holiday   \n",
       "8756                       0            0             0  Autumn  No Holiday   \n",
       "8757                       0            0             0  Autumn  No Holiday   \n",
       "8758                       0            0             0  Autumn  No Holiday   \n",
       "8759                       0            0             0  Autumn  No Holiday   \n",
       "\n",
       "     Functioning Day  \n",
       "0                Yes  \n",
       "1                Yes  \n",
       "2                Yes  \n",
       "3                Yes  \n",
       "4                Yes  \n",
       "...              ...  \n",
       "8755             Yes  \n",
       "8756             Yes  \n",
       "8757             Yes  \n",
       "8758             Yes  \n",
       "8759             Yes  \n",
       "\n",
       "[8760 rows x 14 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Measurement date|\n",
      "+----------------+\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-01      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-02      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-03      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-04      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-05      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-06      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-07      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-08      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-09      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-10      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-11      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-12      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-13      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-14      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-15      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-16      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-17      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-18      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-19      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-20      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-21      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-22      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-23      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-24      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-25      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-26      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-27      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-28      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-29      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-30      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-01-31      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-01      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-02      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-03      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-04      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-05      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-06      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-07      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-08      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-09      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-10      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-11      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-12      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-13      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-14      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "|2017-02-15      |\n",
      "+----------------+\n",
      "only showing top 1100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_pyspark_pollution.show()\n",
    "Seoul_YMD =  df_pyspark_pollution.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 10)).withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.port.maxRetries\", \"50\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "os.listdir('dags/AirPollutionSeoul')\n",
    "dataset_path = 'dags/AirPollutionSeoul/Measurement_summary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('dags/AirPollutionSeoul/Measurement_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple ways without using option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(dataset_path,header=True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select(['Measurement date','Station code']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (277003214.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    kaggle datasets download -d saurabhshahane/seoul-bike-sharing-demand-prediction\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding and drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('ExtreamS02',df_pyspark['SO2']+'0.004')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "df_pyspark.drop(\"ExtreamS02\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumnRenamed(\"PM2.5\",\"PM25\")\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Handling Missing Values ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Dropping Columns</li>\n",
    "<li>Dropping Rows</li>\n",
    "<li>Various Parameter in Dropping functionalities</li>\n",
    "<li>Handelling with missing values by Mean/Median/Mode</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.drop(how=\"all\")\n",
    "#drop just only when all of column is null\n",
    "df_pyspark.na.drop(how=\"any\",thresh=2)\n",
    "#drop just only when there is values 2 \n",
    "df_pyspark.na.drop(how=\"any\",subset=[\"PM25\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select([count(when(isnan(c), c)).alias(c) for c in df_pyspark.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### filling the missing value\n",
    "## this is how we fill the missing value by select particular columns\n",
    "df_pyspark.na.fill('Missing vaue',[\"Address\",\"SO2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillter Operation (SQL)\n",
    "<li> Filter operations</li>\n",
    "<li> &,|,==</li>\n",
    "<li> ~ </li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(\"PM25 <= 50\").select([\"Address\",\"Measurement date\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# & Operator\n",
    "df_pyspark.filter( (df_pyspark[\"PM25\"]<=50) & (df_pyspark[\"PM10\"]>=90))\\\n",
    "    .select([\"PM25\",\"PM10\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(~(df_pyspark[\"PM25\"]<= 50)).select(\"PM25\").show()\n",
    "# this is nothing other than inverse fillter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('test').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark = spark.read.option('header','true').csv('dags/AirPollutionSeoul/Measurement_summary.csv')\n",
    "df_pyspark = spark.read.csv(\n",
    "    path = \"dags/AirPollutionSeoul/Measurement_summary.csv\",\n",
    "    header = True,\n",
    "    )\n",
    "df_pyspark = df_pyspark.withColumnRenamed(\"PM2.5\",\"PM25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select[\"SO2\",\"NO2\",\"O3\",\"CO\",\"PM10\",\"PM25\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn(\"PM25\",df_pyspark[\"PM25\"].cast('float').alias(\"PM25\")\n",
    ")\n",
    "df_pyspark = df_pyspark.withColumn(\"PM10\",df_pyspark[\"PM10\"].cast('float').alias(\"PM10\")\n",
    ")\n",
    "df_pyspark = df_pyspark.withColumn(\"SO2\",df_pyspark[\"SO2\"].cast('float').alias(\"SO2\")\n",
    ")\n",
    "df_pyspark = df_pyspark.withColumn(\"CO\",df_pyspark[\"CO\"].cast('float').alias(\"CO\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df_pyspark.summary().cache()\n",
    "df_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(df_stat.collect()[1][\"PM25\"])+(3*float(df_stat.collect()[2][\"PM25\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*df_stat.collect()[2][\"PM25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.agg({\"PM25\":'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy(\"Address\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_test =df_pyspark.select([\"SO2\",\"PM25\"])\n",
    "df_pyspark_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler =  VectorAssembler(inputCols=[\"SO2\",\"CO\",\"PM10\"],outputCol=\"features\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df_pyspark).select([\"SO2\",\"CO\",\"PM10\",\"PM25\",\"features\"])\n",
    "output = output.select([\"PM25\",\"features\"])\n",
    "output.show()\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = output.randomSplit([0.75,0.25])\n",
    "model = LinearRegression(featuresCol=\"features\",labelCol=\"PM25\")\n",
    "model = model.fit(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = output.randomSplit([0.75, 0.25])  # Adding a seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = output.randomSplit([0.75,0.25])\n",
    "model = LinearRegression(featuresCol=\"features\",labelCol=\"PM25\")\n",
    "model = model.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas =df_pyspark.pandas_api()\n",
    "df_pandas[\"PM25\"].hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "num = df_pandas[\"SO2\"][:].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(num)\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas[\"PM25\"][:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.evaluate(test_data)\n",
    "predict.predictions.collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (0.004, 1.2, 73.0, 57.0),\n",
    "    (0.004, 1.2, 71.0, 5.0),\n",
    "    (0.004, 1.2, 70.0, 59.0),\n",
    "    (0.004, 1.2, 70.0, 58.0),\n",
    "    (0.003, 1.2, 69.0, 61.0)\n",
    "]\n",
    "columns = ['SO2', 'CO', 'PM10', 'PM25']\n",
    "\n",
    "# Create DataFrame\n",
    "df_pyspark = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['SO2', 'CO', 'PM10'],\n",
    "    outputCol='features'\n",
    ")\n",
    "output = assembler.transform(df_pyspark)\n",
    "\n",
    "# Select relevant columns\n",
    "finalized = output.select([\"SO2\", \"CO\", \"PM10\", \"PM25\", \"features\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "finalized.show(truncate=False)\n",
    "\n",
    "# Perform random split\n",
    "a, b = finalized.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = finalized.randomSplit([0.75, 0.25], seed=42)\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_pyspark.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sqlspark_data():\n",
    "    from pyspark.sql.functions import col, split, trim, substring \n",
    "    spark = SparkSession.builder.appName('practise').getOrCreate() \n",
    "    # df_pyspark = spark.read.option('header','true').csv('dags/AirPollutionSeoul/Measurement_summary.csv')\n",
    "    df_pyspark = spark.read.csv(\n",
    "    path = \"./AirPollutionSeoul/Measurement_summary.csv\",\n",
    "    header = True,\n",
    "    )\n",
    "    df_pyspark = df_pyspark.withColumnRenamed(\"PM2.5\",\"PM25\")\n",
    "    columns_to_cast = [\"PM25\", \"PM10\", \"SO2\", \"CO\", \"NO2\", \"O3\"]\n",
    "\n",
    "    for col_name in columns_to_cast:\n",
    "        df_pyspark = df_pyspark.withColumn(col_name, df_pyspark[col_name].cast('float'))\n",
    "\n",
    "    df_stat = df_pyspark.summary().cache()\n",
    "    pm25_cutoff_max = float(df_stat.collect()[1][\"PM25\"])+(2*float(df_stat.collect()[2][\"PM25\"]))\n",
    "    pm10_cutoff_max = float(df_stat.collect()[1][\"PM10\"])+(2*float(df_stat.collect()[2][\"PM10\"]))\n",
    "    so2_cutoff_max  = float(df_stat.collect()[1][\"SO2\"])+(2*float(df_stat.collect()[2][\"SO2\"]))\n",
    "    co_cutoff_max   = float(df_stat.collect()[1][\"CO\"])+(2*float(df_stat.collect()[2][\"CO\"]))\n",
    "    no2_cutoff_max  = float(df_stat.collect()[1][\"NO2\"])+(2*float(df_stat.collect()[2][\"NO2\"]))\n",
    "    o3_cutoff_max   = float(df_stat.collect()[1][\"O3\"])+(2*float(df_stat.collect()[2][\"O3\"]))\n",
    "\n",
    "    pm25_cutoff_min = float(df_stat.collect()[4][\"PM25\"])/2\n",
    "    pm10_cutoff_min = float(df_stat.collect()[4][\"PM10\"])/2\n",
    "    so2_cutoff_min = float(df_stat.collect()[4][\"SO2\"])/2\n",
    "    co_cutoff_min  = float(df_stat.collect()[4][\"CO\"])/2\n",
    "    no2_cutoff_min  = float(df_stat.collect()[4][\"NO2\"])/2\n",
    "    o3_cutoff_min  = float(df_stat.collect()[4][\"O3\"])/2\n",
    "    \n",
    "\n",
    "    df_pyspark = df_pyspark.filter((df_pyspark[\"PM25\"] >= pm25_cutoff_min )   &   (df_pyspark[\"PM25\"] < pm25_cutoff_max)    &\n",
    "                   (df_pyspark[\"PM10\"] >= pm10_cutoff_min)      &   (df_pyspark[\"PM10\"] < pm10_cutoff_max ) &\n",
    "                   (df_pyspark[\"SO2\"]>= so2_cutoff_min)        &   (df_pyspark[\"SO2\"] < so2_cutoff_max )   &\n",
    "                   (df_pyspark[\"NO2\"]>= no2_cutoff_min)         &   (df_pyspark[\"NO2\"] < no2_cutoff_max )  &\n",
    "                   (df_pyspark[\"O3\"]>=  o3_cutoff_min)          &   (df_pyspark[\"O3\"] < o3_cutoff_max )     &\n",
    "                   (df_pyspark[\"CO\"]>=  co_cutoff_min)          &   (df_pyspark[\"CO\"] < co_cutoff_max ) )\n",
    "    \n",
    "\n",
    "    df_pyspark =  df_pyspark.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 4))\n",
    "    df_pyspark =  df_pyspark.withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n",
    "\n",
    "    df_pyspark.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"./Measurement_summary_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practise').getOrCreate() \n",
    "\n",
    "df_pyspark_read = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./Measurement_summary_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read.withColumn(\"Measurement date\",col(\"Measurement date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read.filter(df_pyspark_read[\"Measurement date\"] == 2017).show()\n",
    "df_pyspark_read.filter(df_pyspark_read[\"Measurement date\"] == 2018).show()\n",
    "S2019 = df_pyspark_read.filter(df_pyspark_read[\"Measurement date\"] == 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./S2019\")\n",
    "df_pyspark_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2019.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"./S2019\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, trim, substring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoulmod[\"Address\"]=Seoulmod[\"Address\"].str.split(',').str[2].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read = df_pyspark_read.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 4))\n",
    "df_pyspark_read = df_pyspark_read.withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n",
    "df_pyspark_read.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_read.select(\"Measurement date\").str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=_sqlspark_data()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "assembler =  VectorAssembler(inputCols=[\"SO2\",\"CO\",\"PM10\",\"NO2\"],outputCol=\"features\", handleInvalid=\"keep\")\n",
    "output = assembler.transform(df_pyspark).select([\"PM25\",\"features\"])\n",
    "output = output.select([\"PM25\",\"features\"])\n",
    "train_data, test_data = output.randomSplit([0.75,0.25],seed=1)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "model = LinearRegression(featuresCol=\"features\",labelCol=\"PM25\")\n",
    "model = model.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = model.summary\n",
    "trainingSummary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.evaluate(test_data)\n",
    "predict.predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark.filter(\"PM25 <= 0\").select([\"PM25\"]).show()\n",
    "# df_pyspark.filter(~(df_pyspark[\"SO2\"] <= 0)).select([\"SO2\"]).show()\n",
    "df_pyspark.filter((df_pyspark[\"PM25\"] > 0 ) &\n",
    "                   (df_pyspark[\"PM10\"] > 0 )& \n",
    "                   (df_pyspark[\"CO\"]> 0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoulmod[\"Measurement date\"]=Seoulmod[\"Measurement date\"].str.slice(0,4)\n",
    "Seoulmod[\"Address\"]=Seoulmod[\"Address\"].str.split(',').str[2].str.strip()\n",
    "\n",
    "Seoulall =Seoulmod.drop([\"Measurement date\"],axis=1).groupby('Address').mean().reset_index()\n",
    "Seoul2017 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2017\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2018 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2018\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2019 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2019\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _geomerge_plot() :\n",
    "        import geopandas as gpd\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from pyspark.sql import SparkSession \n",
    "        import os\n",
    "\n",
    "        spark = SparkSession.builder.appName('spark').getOrCreate() \n",
    "\n",
    "        # Seoulall = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/all\")\n",
    "        Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2017\")\n",
    "        Seoul2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2018\")\n",
    "        Seoul2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2019\")\n",
    "        tempSeoulGep= gpd.read_file(\"/Users/art/Airflow/plugins/seoul_municipalities_geo.json\")\n",
    "\n",
    "        Seoul2017_pd = Seoul2017.toPandas()\n",
    "        Seoul2018_pd = Seoul2018.toPandas()\n",
    "        Seoul2019_pd = Seoul2019.toPandas()\n",
    "        # Seoulall_pd = Seoulall.toPandas()\n",
    "\n",
    "\n",
    "        SeoulGeo_pollution2017 = tempSeoulGep.merge(Seoul2017_pd, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "        SeoulGeo_pollution2018 = tempSeoulGep.merge(Seoul2018_pd, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "        SeoulGeo_pollution2019 = tempSeoulGep.merge(Seoul2019_pd, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "\n",
    "\n",
    "        f, axes = plt.subplots(figsize=(1, 2), ncols=2, nrows=1,layout=\"compressed\")\n",
    "        SeoulGeo_pollution2017.plot(ax=axes[0], column='PM25', cmap='OrRd', legend=True)\n",
    "        SeoulGeo_pollution2017.plot(ax=axes[1], column='PM10', cmap='OrRd', legend=True)\n",
    "        # SeoulGeo_pollution2017.plot(ax=axes[2][0], column='CO', cmap='OrRd', legend=True,vmin = mindata[\"CO\"],vmax = maxdata[\"CO\"])\n",
    "        # SeoulGeo_pollution2017.plot(ax=axes[3][0], column='SO2', cmap='OrRd', legend=True,vmin = mindata[\"SO2\"],vmax = maxdata[\"SO2\"])\n",
    "        # SeoulGeo_pollution2018.plot(ax=axes[0][1], column='PM25 ', cmap='OrRd', legend=True,vmin = mindata[\"PM25\"],vmax = maxdata[\"PM25\"])\n",
    "        # SeoulGeo_pollution2018.plot(ax=axes[1][1], column='PM10', cmap='OrRd', legend=True,vmin = mindata[\"PM10\"],vmax = maxdata[\"PM10\"])\n",
    "        # SeoulGeo_pollution2018.plot(ax=axes[2][1], column='CO', cmap='OrRd', legend=True,vmin = mindata[\"CO\"],vmax = maxdata[\"CO\"])\n",
    "        # SeoulGeo_pollution2018.plot(ax=axes[3][1], column='SO2', cmap='OrRd', legend=True,vmin = mindata[\"SO2\"],vmax = maxdata[\"SO2\"])\n",
    "        # SeoulGeo_pollution2019.plot(ax=axes[0][2], column='PM25', cmap='OrRd', legend=True,vmin = mindata[\"PM25\"],vmax = maxdata[\"PM25\"])\n",
    "        # SeoulGeo_pollution2019.plot(ax=axes[1][2], column='PM10', cmap='OrRd', legend=True,vmin = mindata[\"PM10\"],vmax = maxdata[\"PM10\"])\n",
    "        # SeoulGeo_pollution2019.plot(ax=axes[2][2], column='CO', cmap='OrRd', legend=True,vmin = mindata[\"CO\"],vmax = maxdata[\"CO\"])\n",
    "        # SeoulGeo_pollution2019.plot(ax=axes[3][2], column='SO2', cmap='OrRd', legend=True,vmin = mindata[\"SO2\"],vmax = maxdata[\"SO2\"])\n",
    "\n",
    "        # for i, ax_row in enumerate(axes):\n",
    "        #     for j, ax in enumerate(ax_row):\n",
    "        #         ax.set_xticks([])\n",
    "        #         ax.set_yticks([])\n",
    "        #         if i ==0:\n",
    "        #             if j == 0:\n",
    "        #                 ax.set_title(\"2017\" , fontsize=12)\n",
    "        #             elif j==1:\n",
    "        #                 ax.set_title(\"2018\" , fontsize=12)\n",
    "        #             else:\n",
    "        #                 ax.set_title(\"2019\" , fontsize=12)\n",
    "        #         else:\n",
    "        #             ax.set_title(\" \")\n",
    "        # plt.savefig(\"./geo_map.png\")\n",
    "\n",
    "def _sqlspark_data():\n",
    "    spark = SparkSession.builder.appName('practise').getOrCreate() \n",
    "    # df_pyspark = spark.read.option('header','true').csv('dags/AirPollutionSeoul/Measurement_summary.csv')\n",
    "    df_pyspark = spark.read.csv(\n",
    "    path = \"./AirPollutionSeoul/Measurement_summary.csv\",\n",
    "    header = True,\n",
    "    )\n",
    "    df_pyspark = df_pyspark.withColumnRenamed(\"PM2.5\",\"PM25\")\n",
    "    columns_to_cast = [\"PM25\", \"PM10\", \"SO2\", \"CO\", \"NO2\", \"O3\"]\n",
    "\n",
    "    for col_name in columns_to_cast:\n",
    "        df_pyspark = df_pyspark.withColumn(col_name, df_pyspark[col_name].cast('float'))\n",
    "\n",
    "    df_stat = df_pyspark.summary().cache()\n",
    "    pm25_cutoff_max = float(df_stat.collect()[1][\"PM25\"])+(2*float(df_stat.collect()[2][\"PM25\"]))\n",
    "    pm10_cutoff_max = float(df_stat.collect()[1][\"PM10\"])+(2*float(df_stat.collect()[2][\"PM10\"]))\n",
    "    so2_cutoff_max  = float(df_stat.collect()[1][\"SO2\"])+(2*float(df_stat.collect()[2][\"SO2\"]))\n",
    "    co_cutoff_max   = float(df_stat.collect()[1][\"CO\"])+(2*float(df_stat.collect()[2][\"CO\"]))\n",
    "    no2_cutoff_max  = float(df_stat.collect()[1][\"NO2\"])+(2*float(df_stat.collect()[2][\"NO2\"]))\n",
    "    o3_cutoff_max   = float(df_stat.collect()[1][\"O3\"])+(2*float(df_stat.collect()[2][\"O3\"]))\n",
    "\n",
    "    pm25_cutoff_min = float(df_stat.collect()[4][\"PM25\"])/2\n",
    "    pm10_cutoff_min = float(df_stat.collect()[4][\"PM10\"])/2\n",
    "    so2_cutoff_min = float(df_stat.collect()[4][\"SO2\"])/2\n",
    "    co_cutoff_min  = float(df_stat.collect()[4][\"CO\"])/2\n",
    "    no2_cutoff_min  = float(df_stat.collect()[4][\"NO2\"])/2\n",
    "    o3_cutoff_min  = float(df_stat.collect()[4][\"O3\"])/2\n",
    "    \n",
    "\n",
    "    df_pyspark = df_pyspark.filter((df_pyspark[\"PM25\"] >= pm25_cutoff_min )   &   (df_pyspark[\"PM25\"] < pm25_cutoff_max)    &\n",
    "                   (df_pyspark[\"PM10\"] >= pm10_cutoff_min)      &   (df_pyspark[\"PM10\"] < pm10_cutoff_max ) &\n",
    "                   (df_pyspark[\"SO2\"]>= so2_cutoff_min)         &   (df_pyspark[\"SO2\"] < so2_cutoff_max )   &\n",
    "                   (df_pyspark[\"NO2\"]>= no2_cutoff_min)         &   (df_pyspark[\"NO2\"] < no2_cutoff_max )   &\n",
    "                   (df_pyspark[\"O3\"]>=  o3_cutoff_min)          &   (df_pyspark[\"O3\"] < o3_cutoff_max )     &\n",
    "                   (df_pyspark[\"CO\"]>=  co_cutoff_min)          &   (df_pyspark[\"CO\"] < co_cutoff_max ) )\n",
    "    \n",
    "\n",
    "    df_pyspark =  df_pyspark.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 4))\n",
    "    df_pyspark =  df_pyspark.withColumn(\"Address\", trim(split(col(\"Address\"), \",\").getItem(2)))\n",
    "    Seoul2017 = df_pyspark.filter(df_pyspark[\"Measurement date\"] == 2017)\n",
    "    Seoul2018 = df_pyspark.filter(df_pyspark[\"Measurement date\"] == 2018)\n",
    "    Seoul2019 = df_pyspark.filter(df_pyspark[\"Measurement date\"] == 2019)\n",
    "\n",
    "    Seoul2017.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"2017\")\n",
    "    Seoul2018.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"2018\")\n",
    "    Seoul2019.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"2019\")\n",
    "    df_pyspark.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate() \n",
    "\n",
    "# Seoulall = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"all\")\n",
    "Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2017\")\n",
    "max_pm25 = Seoul2017.select(F.max(\"PM25\")).collect()[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoul201\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 16:38:58 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName('spark') \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 18:05:12 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:05:13 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load data from CSV files into Spark DataFrames\u001b[39;00m\n\u001b[1;32m     12\u001b[0m Seoul_all \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/art/Airflow/dags/AirPollutionSeoul/Seoul_MY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "# Load data from CSV files into Spark DataFrames\n",
    "Seoul_all = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/Seoul_MY\")\n",
    "Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2017\")\n",
    "Seoul2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2018\")\n",
    "Seoul2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/2019\")\n",
    "\n",
    "\n",
    "pollution_columns = ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM25']\n",
    "for col in pollution_columns:\n",
    "    Seoul_all = Seoul_all.withColumn(col, Seoul_all[col].cast('float'))\n",
    "    Seoul2017 = Seoul2017.withColumn(col, Seoul2017[col].cast('float'))\n",
    "    Seoul2018 = Seoul2018.withColumn(col, Seoul2018[col].cast('float'))\n",
    "    Seoul2019 = Seoul2019.withColumn(col, Seoul2019[col].cast('float'))\n",
    "# Group by 'Address' and calculate the average for each pollution column\n",
    "\n",
    "# # Load GeoJSON data into a GeoDataFrame\n",
    "tempSeoulGep = gpd.read_file(\"/Users/art/Airflow/plugins/seoul_municipalities_geo.json\")\n",
    "\n",
    "\n",
    "Seoul_all = Seoul_all.toPandas()\n",
    "Seoul2017 = Seoul2017.toPandas()\n",
    "Seoul2018 = Seoul2018.toPandas()\n",
    "Seoul2019 = Seoul2019.toPandas()\n",
    "\n",
    "\n",
    "Seoul_all =Seoul_all.drop([\"Measurement date\",\"Station code\" , \"Station code\",\"Address\",\"Latitude\",\"Longitude\"],axis=1)\n",
    "Seoul2017 = Seoul2017[Seoul2017[\"Measurement date\"]==\"2017\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2018 = Seoul2018[Seoul2018[\"Measurement date\"]==\"2018\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2019 = Seoul2019[Seoul2019[\"Measurement date\"]==\"2019\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "maxdata=Seoul_all.iloc[:,0:].max()\n",
    "mindata=Seoul_all.iloc[:,0:].min()\n",
    "\n",
    "SeoulGeo_pollution2017 = tempSeoulGep.merge(Seoul2017, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\", axis=1)\n",
    "SeoulGeo_pollution2018 = tempSeoulGep.merge(Seoul2018, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\", axis=1)\n",
    "SeoulGeo_pollution2019 = tempSeoulGep.merge(Seoul2019, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Pollution_key = [\"PM25\", \"PM10\",\"SO2\",\"CO\"]\n",
    "f, axes = plt.subplots(figsize=(5, 15), ncols = 3, nrows = 4,layout=\"compressed\")\n",
    "\n",
    "for i,keys in enumerate(Pollution_key): \n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    SeoulGeo_pollution2017.plot(ax=axes[i][0], column= keys, cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": \"\", \"orientation\": \"horizontal\"})\n",
    "    SeoulGeo_pollution2018.plot(ax=axes[i][1], column=keys, cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": keys, \"orientation\": \"horizontal\"})\n",
    "    SeoulGeo_pollution2019.plot(ax=axes[i][2], column= keys , cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": \"\", \"orientation\": \"horizontal\"} ,)\n",
    "\n",
    "for i, ax_row in enumerate(axes):\n",
    "    for j, ax in enumerate(ax_row):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if i ==0:\n",
    "            if j == 0:\n",
    "                ax.set_title(\"2017\" , fontsize=12)\n",
    "            elif j==1:\n",
    "                ax.set_title(\"2018\" , fontsize=12)\n",
    "            else:\n",
    "                ax.set_title(\"2019\" , fontsize=12)\n",
    "        else:\n",
    "            ax.set_title(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x1500 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "Pollution_key = [\"PM25\", \"PM10\",\"SO2\",\"CO\"]\n",
    "f, axes = plt.subplots(figsize=(5, 15), ncols = 3, nrows = 4,layout=\"compressed\")\n",
    "\n",
    "for i,keys in enumerate(Pollution_key): \n",
    "    vmin = SeoulGeo_pollution2017[keys].min()\n",
    "    vmax = SeoulGeo_pollution2017[keys].max()\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    SeoulGeo_pollution2017.plot(ax=axes[i][0], column= keys, cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": \"\", \"orientation\": \"horizontal\"})\n",
    "    SeoulGeo_pollution2018.plot(ax=axes[i][1], column=keys, cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": keys, \"orientation\": \"horizontal\"})\n",
    "    SeoulGeo_pollution2019.plot(ax=axes[i][2], column= keys , cmap='OrRd', legend=True, \n",
    "                                legend_kwds={\"label\": \"\", \"orientation\": \"horizontal\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoul2017_pd = grouped_df.toPandas()\n",
    "SeoulGeo_pollution2017_spark = tempSeoulGep.merge(Seoul2017_pd, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\", axis=1)\n",
    "SeoulGeo_pollution2017_spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.7</td>\n",
       "      <td>76.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.7</td>\n",
       "      <td>80.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.6</td>\n",
       "      <td>71.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.8</td>\n",
       "      <td>73.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.9</td>\n",
       "      <td>66.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473007</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473008</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473009</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473010</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473011</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473012 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SO2    NO2     O3   CO  PM10  PM25\n",
       "0       0.011  0.044  0.035  0.7  76.0  49.0\n",
       "1       0.014  0.050  0.034  0.7  80.0  52.0\n",
       "2       0.011  0.058  0.034  0.6  71.0  47.0\n",
       "3       0.013  0.077  0.007  0.8  73.0  49.0\n",
       "4       0.014  0.071  0.004  0.9  66.0  48.0\n",
       "...       ...    ...    ...  ...   ...   ...\n",
       "473007  0.003  0.028  0.013  0.5  23.0  17.0\n",
       "473008  0.003  0.025  0.015  0.4  25.0  19.0\n",
       "473009  0.003  0.023  0.015  0.4  24.0  17.0\n",
       "473010  0.003  0.040  0.004  0.5  25.0  18.0\n",
       "473011  0.003  0.037  0.005  0.5  27.0  18.0\n",
       "\n",
       "[473012 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seoul_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Convert the columns to numeric types (if they are read as strings)\n",
    "pollution_columns = ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM25']\n",
    "for col in pollution_columns:\n",
    "    Seoul2017 = Seoul2017.withColumn(col, Seoul2017[col].cast('float'))\n",
    "\n",
    "# Group by 'Address' and calculate the average for each pollution column\n",
    "grouped_df = Seoul2017.groupBy('Address').agg(\n",
    "    avg('SO2').alias('avg_SO2'),\n",
    "    avg('NO2').alias('avg_NO2'),\n",
    "    avg('O3').alias('avg_O3'),\n",
    "    avg('CO').alias('avg_CO'),\n",
    "    avg('PM10').alias('avg_PM10'),\n",
    "    avg('PM25').alias('avg_PM25')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoul2017_pd = grouped_df.toPandas()\n",
    "SeoulGeo_pollution2017_spark = tempSeoulGep.merge(Seoul2017_pd, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\", axis=1)\n",
    "SeoulGeo_pollution2017_spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Seoul = pd.read_csv(\"/Users/art/Airflow/dags/AirPollutionSeoul/Measurement_summary.csv\")\n",
    "Seoulmod = Seoul.copy()\n",
    "Seoulmod[\"Measurement date\"]=Seoulmod[\"Measurement date\"].str.slice(0,4)\n",
    "Seoulmod[\"Address\"]=Seoulmod[\"Address\"].str.split(',').str[2].str.strip()\n",
    "\n",
    "Seoulall =Seoulmod.drop([\"Measurement date\"],axis=1).groupby('Address').mean().reset_index()\n",
    "Seoul2017 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2017\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2018 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2018\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "Seoul2019 = Seoulmod[Seoulmod[\"Measurement date\"]==\"2019\"].drop(\"Measurement date\",axis=1)\\\n",
    "    .groupby([\"Station code\",\"Address\",\"Latitude\",\"Longitude\"]).mean().reset_index()\n",
    "\n",
    "tempSeoulGep=gpd.read_file(\"/Users/art/Airflow/plugins/seoul_municipalities_geo.json\")\n",
    "SeoulGeo_pollution2017 = tempSeoulGep.merge(Seoul2017, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "SeoulGeo_pollution2018 = tempSeoulGep.merge(Seoul2018, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "SeoulGeo_pollution2019 = tempSeoulGep.merge(Seoul2019, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)\n",
    "maxdata=Seoulall.iloc[:,1:].max()\n",
    "mindata=Seoulall.iloc[:,1:].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeoulGeo_pollution2017_spark.plot(column='avg_PM25', cmap='OrRd', legend=True, legend_kwds={\"label\": \"\", \"orientation\": \"horizontal\"},vmin = 0,vmax = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate() \n",
    "\n",
    "Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2017\")\n",
    "Seoul2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2018\")\n",
    "Seoul2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2019\")\n",
    "tempSeoulGep = spark.read.json(\"../plugins/seoul_municipalities_geo.json\")\n",
    "\n",
    "SeoulGeo_pollution2017 = tempSeoulGep.join(Seoul2017, tempSeoulGep['SIG_ENG_NM']== Seoul2017[\"Address\"], \"inner\").drop(\"Address\")\n",
    "# SeoulGeo_pollution2018 = tempSeoulGep.join(Seoul2018, tempSeoulGep['SIG_ENG_NM']==Seoul2018[\"Address\"], \"inner\").drop(\"Address\")\n",
    "# SeoulGeo_pollution2019 = tempSeoulGep.join(Seoul2019, tempSeoulGep['SIG_ENG_NM']== Seoul2019[\"Address\"], \"inner\").drop(\"Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2017\").pandas_api()\n",
    "tempSeoulGep=gpd.read_file(\"../plugins/seoul_municipalities_geo.json\")\n",
    "\n",
    "# SeoulGeo_pollution2017 = tempSeoulGep.merge(Seoul2017, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempSeoulGep.merge(Seoul2017, left_on='SIG_ENG_NM', right_on='Address').drop(\"Address\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "\n",
    "Seoul2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.DataFrame(Seoul2017).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, trim, substring \n",
    "\n",
    "spark_1 = SparkSession.builder.appName('test').getOrCreate() \n",
    "# Seoul = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/opt/airflow/dags/AirPollutionSeoul/all\").drop([\"Latitude\",\"Longitude\",\"Station code\"])\n",
    "\n",
    "\n",
    "# Seoul = Seoul.withColumn(\"M\", substring(\"Measurement date\", 6, 2))\n",
    "# Seoul = Seoul.withColumn(\"Y\", substring(\"Measurement date\", 1, 4))\n",
    "# Seoul= Seoul.drop(\"Measurement date\")\n",
    "\n",
    "# pre_plot =  Seoul.drop(\"Y\").groupby([\"Address\",\"M\"]).mean()\n",
    "# plot_temp= pre_plot.drop([\"Address\",\"M\"],axis=1)\n",
    "# fig1 = plt.figure(figsize=(12,5))\n",
    "# corr= plot_temp.corr()\n",
    "# matrix = np.triu(corr)\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# sns.heatmap(corr,annot=True, mask=matrix, cmap=cmap)\n",
    "# plt.savefig(\"/opt/airflow/dags/correlation1.png\")\n",
    "\n",
    "# fig,axes = plt.subplots(figsize=(6, 6), ncols=2, nrows=3,layout=\"compressed\")\n",
    "# sns.regplot(x=\"PM2.5\",y=\"PM10\",data = plot_temp, ax=axes[0,0],line_kws={\"color\": \"red\"})\n",
    "# sns.residplot(x=\"PM2.5\",y=\"PM10\",data = plot_temp, ax=axes[0,1])\n",
    "# sns.regplot(x=\"O3\",y=\"CO\",data = plot_temp, ax=axes[1,0],line_kws={\"color\": \"red\"})\n",
    "# sns.residplot(x=\"O3\",y=\"CO\",data = plot_temp, ax=axes[1,1])\n",
    "# sns.regplot(x=\"O3\",y=\"SO2\",data = plot_temp, ax=axes[2,0],line_kws={\"color\": \"red\"})\n",
    "# sns.residplot(x=\"O3\",y=\"SO2\",data = plot_temp, ax=axes[2,1])\n",
    "# plt.savefig(\"/opt/airflow/dags/correlation2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, trim, substring \n",
    "\n",
    "spark = SparkSession.builder.appName('corelation_analysis').getOrCreate() \n",
    "Seoul = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./AirPollutionSeoul/Seoul_MY\").drop(\"Latitude\",\"Longitude\",\"Station code\")\n",
    "\n",
    "\n",
    "# Seoul = Seoul.withColumn(\"M\", substring(\"Measurement date\", 5, 2))\n",
    "# Seoul = Seoul.withColumn(\"Y\", substring(\"Measurement date\", 1, 4))\n",
    "# Seoul= Seoul.drop(\"Measurement date\")\n",
    "\n",
    "Seoul_MY =  Seoul.withColumn(\"Measurement date\", substring(col(\"Measurement date\"), 1, 7))\n",
    "Seoul = Seoul.withColumn(\"M\", substring(\"Measurement date\", 6, 2).cast(\"int\"))\n",
    "Seoul = Seoul.withColumn(\"Y\", substring(\"Measurement date\", 1, 4).cast(\"int\"))\n",
    "\n",
    "Seoul.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_plot =  Seoul.drop(\"Y\").groupby(\"Address\",\"M\",\"PM25\").mean()\n",
    "# plot_temp= pre_plot.drop([\"Address\",\"M\"])\n",
    "# fig1 = plt.figure(figsize=(12,5))\n",
    "# corr= plot_temp.corr()\n",
    "# matrix = np.triu(corr)\n",
    "# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# sns.heatmap(corr,annot=True, mask=matrix, cmap=cmap)\n",
    "\n",
    "pre_plot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# List all columns except \"Address\" and \"M\"\n",
    "columns_to_avg = [c for c in Seoul.columns if c not in [\"Address\", \"M\"]]\n",
    "\n",
    "# Create a list of average aggregations for the columns\n",
    "avg_expressions = [avg(col(c)).alias(f\"{c}\") for c in columns_to_avg]\n",
    "\n",
    "# Perform groupBy and aggregation\n",
    "pre_plot = Seoul.groupBy(\"Address\", \"M\").agg(*avg_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_plot =  Seoul.drop(\"Y\").groupby([\"Address\",\"M\"]).mean()\n",
    "plot_temp= pre_plot.drop([\"Address\",\"M\"],axis=1)\n",
    "fig1 = plt.figure(figsize=(12,5))\n",
    "corr= plot_temp.corr()\n",
    "matrix = np.triu(corr)\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr,annot=True, mask=matrix, cmap=cmap)\n",
    "plt.savefig(\"/opt/airflow/dags/correlation1.png\")\n",
    "\n",
    "fig,axes = plt.subplots(figsize=(6, 6), ncols=2, nrows=3,layout=\"compressed\")\n",
    "sns.regplot(x=\"PM2.5\",y=\"PM10\",data = plot_temp, ax=axes[0,0],line_kws={\"color\": \"red\"})\n",
    "sns.residplot(x=\"PM2.5\",y=\"PM10\",data = plot_temp, ax=axes[0,1])\n",
    "sns.regplot(x=\"O3\",y=\"CO\",data = plot_temp, ax=axes[1,0],line_kws={\"color\": \"red\"})\n",
    "sns.residplot(x=\"O3\",y=\"CO\",data = plot_temp, ax=axes[1,1])\n",
    "sns.regplot(x=\"O3\",y=\"SO2\",data = plot_temp, ax=axes[2,0],line_kws={\"color\": \"red\"})\n",
    "sns.residplot(x=\"O3\",y=\"SO2\",data = plot_temp, ax=axes[2,1])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/users/art/Desktop/data.csv\", encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"InvoiceDate\"] =df[\"InvoiceDate\"].str.slice(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =df.groupby(\"InvoiceDate\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test[\"InvoiceDate\"],test[\"Quantity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test[\"Quantity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "Seoulmod = pd.read_csv(\"/Users/art/Airflow/dags/AirPollutionSeoul/Measurement_summary.csv\")\n",
    "Seoulmod[\"Measurement date\"]=Seoulmod[\"Measurement date\"].str.slice(0,7)\n",
    "Seoulmod[\"Address\"]=Seoulmod[\"Address\"].str.split(',').str[2].str.strip()\n",
    "Seoulmod = Seoulmod.drop([\"Latitude\",\"Longitude\",\"Station code\"],axis=1)\n",
    "Seoulmod.insert(0,\"M\",Seoulmod[\"Measurement date\"].str.slice(5,7),True)\n",
    "Seoulmod.insert(1,\"Y\",Seoulmod[\"Measurement date\"].str.slice(0,4),True)\n",
    "Seoulmod.drop(\"Measurement date\",axis=1,inplace=True)\n",
    "pre_plot = Seoulmod.drop([\"Y\"],axis=1).groupby([\"Address\",\"M\"],as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"/Users/art/Airflow/dags/AirPollutionSeoul/Seoul_MY/part-00000-4c4c62b1-ff6f-4f6f-8c6f-88d468189387-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>Measurement date</th>\n",
       "      <th>Station code</th>\n",
       "      <th>Address</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>101</td>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>101</td>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>101</td>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>101</td>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>101</td>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>37.572016</td>\n",
       "      <td>127.005008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.9</td>\n",
       "      <td>93.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44257</th>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>103</td>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>37.540033</td>\n",
       "      <td>127.004850</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44258</th>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>103</td>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>37.540033</td>\n",
       "      <td>127.004850</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44259</th>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>103</td>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>37.540033</td>\n",
       "      <td>127.004850</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44260</th>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>103</td>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>37.540033</td>\n",
       "      <td>127.004850</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44261</th>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>09</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>103</td>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>37.540033</td>\n",
       "      <td>127.004850</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44262 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        M     Y   M     Y Measurement date  Station code     Address  \\\n",
       "0      01  2017  01  2017          2017-01           101   Jongno-gu   \n",
       "1      01  2017  01  2017          2017-01           101   Jongno-gu   \n",
       "2      01  2017  01  2017          2017-01           101   Jongno-gu   \n",
       "3      01  2017  01  2017          2017-01           101   Jongno-gu   \n",
       "4      01  2017  01  2017          2017-01           101   Jongno-gu   \n",
       "...    ..   ...  ..   ...              ...           ...         ...   \n",
       "44257  09  2017  09  2017          2017-09           103  Yongsan-gu   \n",
       "44258  09  2017  09  2017          2017-09           103  Yongsan-gu   \n",
       "44259  09  2017  09  2017          2017-09           103  Yongsan-gu   \n",
       "44260  09  2017  09  2017          2017-09           103  Yongsan-gu   \n",
       "44261  09  2017  09  2017          2017-09           103  Yongsan-gu   \n",
       "\n",
       "        Latitude   Longitude    SO2    NO2     O3   CO  PM10  PM25  \n",
       "0      37.572016  127.005008  0.004  0.045  0.004  1.0  76.0  68.0  \n",
       "1      37.572016  127.005008  0.004  0.048  0.005  1.0  83.0  74.0  \n",
       "2      37.572016  127.005008  0.004  0.054  0.005  1.0  93.0  79.0  \n",
       "3      37.572016  127.005008  0.005  0.050  0.007  1.0  94.0  79.0  \n",
       "4      37.572016  127.005008  0.006  0.055  0.008  0.9  93.0  78.0  \n",
       "...          ...         ...    ...    ...    ...  ...   ...   ...  \n",
       "44257  37.540033  127.004850  0.004  0.029  0.019  0.3  20.0   9.0  \n",
       "44258  37.540033  127.004850  0.004  0.027  0.023  0.3  19.0  11.0  \n",
       "44259  37.540033  127.004850  0.005  0.029  0.025  0.3  26.0  10.0  \n",
       "44260  37.540033  127.004850  0.004  0.023  0.032  0.2  18.0  12.0  \n",
       "44261  37.540033  127.004850  0.004  0.021  0.035  0.2  25.0  13.0  \n",
       "\n",
       "[44262 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"Measurement date\"]\n",
    "a.insert(0,\"M\",a[\"Measurement date\"].str.slice(5,7),True)\n",
    "a.insert(1,\"Y\",a[\"Measurement date\"].str.slice(0,4),True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/art/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "24/08/29 18:19:53 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('spark') \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "# Load data from CSV files into Spark DataFrames\n",
    "# Seoul_all = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/Seoul_MY\")\n",
    "# Seoul_all = Seoul_all.toPandas()\n",
    "# Seoul_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===============================================>         (10 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "Seoul_all = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Users/art/Airflow/dags/AirPollutionSeoul/Seoul_MY\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 18:06:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "24/08/29 18:07:00 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.julia/conda/3/x86_64/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"a\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
